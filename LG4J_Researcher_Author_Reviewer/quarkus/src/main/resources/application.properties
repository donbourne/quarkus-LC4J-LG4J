# quarkus.langchain4j.chat-model.provider=openai
# quarkus.langchain4j.chat-model.provider=watsonx
quarkus.langchain4j.chat-model.provider=ollama

quarkus.langchain4j.log-requests=false
quarkus.langchain4j.log-responses=false
quarkus.langchain4j.openai.api-key=demo

# quarkus.langchain4j.watsonx.chat-model.model-id=mistralai/mistral-large
# quarkus.langchain4j.watsonx.chat-model.model-id=ibm/granite-3-8b-instruct
quarkus.langchain4j.watsonx.chat-model.model-id=meta-llama/llama-3-1-70b-instruct
quarkus.langchain4j.watsonx.chat-model.temperature=0.0
quarkus.langchain4j.watsonx.timeout=60s
quarkus.langchain4j.watsonx.base-url=https://us-south.ml.cloud.ibm.com
# quarkus.langchain4j.watsonx.project-id=<set using other config source>
# quarkus.langchain4j.watsonx.api-key=<set using other config source>

quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.chat-model.temperature=0.0
quarkus.langchain4j.ollama.base-url=http://localhost:11434
quarkus.langchain4j.ollama.devservices.enabled=false
quarkus.langchain4j.ollama.timeout=20s
